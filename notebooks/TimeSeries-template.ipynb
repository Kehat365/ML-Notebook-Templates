{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>Time series template</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 0:</b> Context\n",
    "*Explain the problem you're trying to solve, the nature of the dataset, the goal of the model, and the impact of the solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 1:</b> Imports and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Libraries</b>\n",
    "*Import all necessary Python libraries (e.g., numpy, pandas, scikit-learn, tensorflow/pytorch for deep learning).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Workflow\n",
    "from sklearn.pipeline import Pipeline\n",
    "import mlflow\n",
    "# import mlflow.sklearn\n",
    "\n",
    "#- Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "\n",
    "#- Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "#- Preprocessing for model fitting\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#- ML models\n",
    "import optuna\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "\n",
    "#- Model assessment for regression\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Configurations</b>\n",
    "*Set any global configurations such as random seed for reproducibility, backend settings for computation libraries, and any project-specific parameters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of our experiments we have used mlflow. You'll need to set up an ML flow server to run this notebook to the end. For that follow the instructions\n",
    "\n",
    "1️⃣ Install MLflow\n",
    "Run the following command in your terminal to install MLflow:\n",
    "\n",
    "pip install mlflow\n",
    "\n",
    "2️⃣ Start the MLflow Tracking Server\n",
    "In your terminal, launch the MLflow UI by running:\n",
    "\n",
    "mlflow ui\n",
    "\n",
    "    The UI will be available at http://localhost:5000.\n",
    "\n",
    "3️⃣ Visualize Results\n",
    "Open http://localhost:5000 in your browser to track metrics, parameters, and model artifacts for all our experiments.\n",
    "\n",
    " Optional: You can specify a different tracking server in the flollowing code cell using:\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://your-server-address:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "#- Show all the column\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#- Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"test_mlflow_setup\")      # This is just to test if the mlflow server is working\n",
    "try:\n",
    "    mlflow.log_param(\"test_param\", \"test_value\")\n",
    "    print(\"MLflow is configured correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"MLflow configuration failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Dataset</b>\n",
    "*If you have several data sets, you need to duplicate the cells below. If your dataset is in a different format, create a new cell and write the code to import it. Don't forget Chat GPT and Google are your friends. ;-)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If your data is in a CSV file use the cell below to import it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \".csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "raw_data = data.copy()\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If your data is in an excel file use the cell below to import it instead.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \".xlsx\"\n",
    "# data = pd.read_excel(data_path)\n",
    "# raw_data = data.copy()\n",
    "# data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 2:</b> Data inspection and cleanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Inspection</b>\n",
    "*The goal of data inspection is to understand the dataset's characteristics, quality, and any potential issues that need addressing before analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_inspection(df: pd.DataFrame, plot_missing_data=True, detect_duplicates=True, include_statistics=True):\n",
    "    \"\"\"\n",
    "        Perform a comprehensive inspection of a pandas DataFrame, including an overview of its size,\n",
    "        data types, presence of null values, detection of duplicate rows, and optional detailed statistics.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pandas.DataFrame): The DataFrame to inspect.\n",
    "        - plot_missing_data (bool): If True, generates a heatmap of missing values. Default is True.\n",
    "        - detect_duplicates (bool): If True, checks for and reports the number of duplicate rows. Default is True.\n",
    "        - include_statistics (bool): If True, includes detailed summary statistics for numeric columns and \n",
    "        the most common values for categorical columns in the returned summary table. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        - pandas.DataFrame: A summary table that includes:\n",
    "            - Unique_values: Number of unique values in each column.\n",
    "            - Data_type: The data type of each column.\n",
    "            - Null_count: The number of missing values in each column.\n",
    "            - Null_percentage: The percentage of missing values in each column.\n",
    "            - (Optional) Most_common: The most frequent value in each categorical column.\n",
    "            - (Optional) Mean, Std, Min, Max: Basic statistics for numeric columns.\n",
    "\n",
    "        Prints:\n",
    "        - Basic DataFrame information, including the number of rows, columns, and data types.\n",
    "        - Columns with missing values and their respective counts.\n",
    "        - Heatmap of missing values if plot_missing_data is True.\n",
    "        - Nullity correlation heatmap if multiple columns contain missing values.\n",
    "        - Number of duplicate rows if detect_duplicates is True.\n",
    "    \"\"\"\n",
    "    #- Basic informations\n",
    "    print(f\"Number of rows: {df.shape[0]}\\nNumber of columns: {df.shape[1]}\\n\")\n",
    "    print(\"Data types:\", df.dtypes, sep='\\n')\n",
    "\n",
    "    #- Missing values\n",
    "    missing_value_cnt = df.isnull().sum()\n",
    "    num_cols_with_missing = len(missing_value_cnt[missing_value_cnt > 0])\n",
    "    if num_cols_with_missing == 0:\n",
    "        print(f\"\\n{'-'*100}\\nThere are no null values in the dataset.\\n{'-'*100}\")\n",
    "        if plot_missing_data:\n",
    "            plt.figure(figsize=(15, 7))\n",
    "            sns.heatmap(df.isna(), cbar=True, cmap='viridis')\n",
    "            plt.title('Heatmap of missing values')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f\"\\n{'-'*100}\\nColumns with null values:\\n\", missing_value_cnt[missing_value_cnt > 0])\n",
    "        if plot_missing_data:\n",
    "            plt.figure(figsize=(15, 7))\n",
    "            sns.heatmap(df.isna(), cbar=True, cmap='viridis')\n",
    "            plt.title('Heatmap of missing values')\n",
    "            plt.show()\n",
    "\n",
    "            if num_cols_with_missing > 1:\n",
    "                missing_value_df = df.iloc[:, [i for i, n in enumerate(np.var(df.isnull(), axis='rows')) if n > 0]]\n",
    "                missing_corr_mat = missing_value_df.isnull().corr()\n",
    "                print(\n",
    "                    \"\"\"\n",
    "                        The correlation heatmap is based on the missingno correlation heatmap. It measures \n",
    "                        nullity correlation: how strongly the presence or absence of one variable affects the \n",
    "                        presence of another. Nullity correlation ranges from -1 (if one variable appears the \n",
    "                        other definitely does not) to 0 (variables appearing or not appearing have no effect \n",
    "                        on one another) to 1 (if one variable appears the other definitely also does).\n",
    "                    \"\"\"\n",
    "                )\n",
    "                plt.figure(figsize=(15, 10))\n",
    "                sns.heatmap(missing_corr_mat, annot=True, cmap='coolwarm')\n",
    "                plt.title(\"Nullity correlation heatmap\")\n",
    "                plt.show()\n",
    "\n",
    "    #- Duplicated rows\n",
    "    if detect_duplicates:\n",
    "        duplicated_rows_cnt = df.duplicated().sum()\n",
    "        if duplicated_rows_cnt > 0:\n",
    "            print(f\"\\n{''*100}\\nNumber of duplicated rows: {duplicated_rows_cnt}\\n{''*100}\")\n",
    "        else:\n",
    "            print(f\"\\n{''*100}\\nThere are no duplicated rows in the dataset.\\n{''*100}\")\n",
    "\n",
    "    #- Summary table\n",
    "    summary_table = pd.DataFrame({\n",
    "        \"Unique_values\": df.nunique(),\n",
    "        \"Data_type\": df.dtypes,\n",
    "        \"Null_count\": df.isnull().sum(),\n",
    "        \"Null_percentage\": (df.isnull().sum() / df.shape[0] * 100).round(2)\n",
    "    })\n",
    "\n",
    "    if include_statistics:\n",
    "        # Most common values in categorical columns\n",
    "        summary_table['Most_common'] = df.select_dtypes(include=['object']).apply(\n",
    "            lambda x: x.mode()[0] if len(x.mode()) > 0 else 'N/A')\n",
    "\n",
    "        # Summary statistics for numeric columns\n",
    "        for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "            summary_table.loc[col, 'Mean'] = df[col].mean()\n",
    "            summary_table.loc[col, 'Std'] = df[col].std()\n",
    "            summary_table.loc[col, 'Min'] = df[col].min()\n",
    "            summary_table.loc[col, 'Max'] = df[col].max()\n",
    "    \n",
    "    return summary_table        # I use a return here for a better visualization. You can replace this line with: \"print(summary_table)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the following cell, modify the parameters according to the particularities of your database. For example, if you have intentional duplicates, it is not necessary to check them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_inspection(data, visualize_nulls=True, detailed_summary=True, check_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(df, plot=True):\n",
    "    \"\"\"\n",
    "    Identify outliers in a pandas DataFrame using Z-score (values more than 3 standard deviations from \n",
    "    the mean) and Interquartile Range (IQR) methods (values below Q1 - 1.92*IQR or above Q3 + 1.92*IQR).\n",
    "    Outliers identified by both methods are combined to provide a comprehensive overview of outliers \n",
    "    in each numeric feature.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): A pandas DataFrame containing the data to be analyzed. Only numeric\n",
    "      columns will be considered for outlier detection.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where each key is the name of a numeric feature in the DataFrame. Each value\n",
    "      is another dictionary containing two keys: 'num_outliers', which is the number of unique outliers\n",
    "      identified in the feature, and 'outliers_index', an array of indices of these outliers.\n",
    "    \"\"\"\n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for column in df.select_dtypes(include=np.number).columns:  # Focus on numeric columns\n",
    "        # Calculate Z-scores\n",
    "        z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "        z_outliers = np.where(z_scores > 3)[0]\n",
    "        \n",
    "        # Calculate IQR\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        iqr_outliers = df[(df[column] < (Q1 - 1.96 * IQR)) | (df[column] > (Q3 + 1.96 * IQR))].index\n",
    "        \n",
    "        # Combine unique outliers from both methods\n",
    "        combined_outliers = np.union1d(z_outliers, iqr_outliers)\n",
    "        \n",
    "        # Optionally plot\n",
    "        if plot:\n",
    "            print(column)\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.boxplot(x=df[column])\n",
    "            plt.title(f'Boxplot of {column} (Outliers highlighted)')\n",
    "            plt.show()\n",
    "\n",
    "        # Summary\n",
    "        outlier_summary[column] = {\n",
    "            'num_outliers': len(combined_outliers),\n",
    "            'outliers_index': combined_outliers\n",
    "        }\n",
    "    \n",
    "    return outlier_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This function does not modify the original DataFrame. I recommend you to inspect the identified outliers and decide on appropriate handling methods such as removal, replacement, or keeping them as is, depending on the analysis requirements and domain knowledge.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_info = identify_outliers(data)\n",
    "print(outlier_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Cleaning</b>\n",
    "*Following the inspection, data cleaning aims to rectify issues identified, improving the dataset's quality and making it suitable for further analysis and modeling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Feature Selection\n",
    "\n",
    "# data.drop(column=[], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Remove duplicates\n",
    "\n",
    "# data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to adapt the following function to your own data. You can add or remove transformations, or modify the way they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data, date_col='date_column'):\n",
    "    \"\"\"\n",
    "    Clean and preprocess data for time series prediction.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data to be cleaned.\n",
    "        date_col (str): The column containing datetime information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned and preprocessed data.\n",
    "    \"\"\"\n",
    "    #- Ensure the date column is in datetime format\n",
    "    data[date_col] = pd.to_datetime(data[date_col], errors='coerce')\n",
    "    \n",
    "    #- Sort data by the date column\n",
    "    data = data.sort_values(by=date_col).reset_index(drop=True)\n",
    "    \n",
    "    #- Handle missing values with forward and backward fill (common for time series)\n",
    "    for col in data.select_dtypes(include=[np.number]).columns:\n",
    "        data[col].fillna(method='ffill', inplace=True)\n",
    "        data[col].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    #- Interpolation for smoother time series (optional)\n",
    "    for col in data.select_dtypes(include=[np.number]).columns:\n",
    "        data[col] = data[col].interpolate(method='linear')\n",
    "\n",
    "    #- Extract time-based features\n",
    "    data['year'] = data[date_col].dt.year\n",
    "    data['month'] = data[date_col].dt.month\n",
    "    data['day'] = data[date_col].dt.day\n",
    "    data['day_of_week'] = data[date_col].dt.dayofweek\n",
    "    data['quarter'] = data[date_col].dt.quarter\n",
    "    data['is_weekend'] = data[date_col].dt.dayofweek >= 5\n",
    "    data['hour'] = data[date_col].dt.hour if data[date_col].dt.hour.nunique() > 1 else 0\n",
    "    \n",
    "    #- Generate lag features (useful for time series forecasting)\n",
    "    lag_features = ['numerical_column']  # Replace with relevant numerical columns\n",
    "    for col in lag_features:\n",
    "        for lag in range(1, 4):  # Create lag features for the last 3 time steps\n",
    "            data[f'{col}_lag_{lag}'] = data[col].shift(lag)\n",
    "    \n",
    "    #- Rolling window features (moving averages, rolling sum, etc.)\n",
    "    for col in lag_features:\n",
    "        data[f'{col}_rolling_mean_3'] = data[col].rolling(window=3).mean()\n",
    "        data[f'{col}_rolling_std_3'] = data[col].rolling(window=3).std()\n",
    "\n",
    "    #- Drop rows with remaining missing values (after creating lags and rolling features)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    #- Encoding categorical variables\n",
    "    categorical_cols = ['categorical_column']  # Replace with actual categorical columns\n",
    "    for col in categorical_cols:\n",
    "        data[col] = data[col].astype('category').cat.codes\n",
    "\n",
    "    #- One-hot encoding (optional)\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cleaning(data, date_col)        # You'll have to identify the column of the dataframe that contain the date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>When deciding between one-hot encoding and simple encoding (label encoding) for categorical data, consider the nature of your categories:\n",
    "\n",
    "- One-Hot Encoding: Use this when your categorical variable does not have a meaningful order or hierarchy. One-hot encoding creates a new binary column for each category, which is ideal for nominal data (e.g., color, city names). This approach prevents the model from assuming a natural ordering between categories, which is helpful to avoid misleading interpretations.\n",
    "\n",
    "- Label Encoding: Use this when your categorical variable has a meaningful order or ranking (ordinal data). Label encoding assigns a unique integer to each category, preserving the order. However, be cautious, as this can introduce unintended ordinal relationships that may not exist in the data.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Handle missing values\n",
    "data['numerical_column'].fillna(data['numerical_column'].mean(), inplace=True)\n",
    "data['categorical_column'].fillna(data['categorical_column'].mode()[0], inplace=True)\n",
    "\n",
    "#- Correct data type\n",
    "data['numeric_column_as_string'] = pd.to_numeric(data['numeric_column_as_string'], errors='coerce')\n",
    "data['date_column'] = pd.to_datetime(data['date_column'])\n",
    "\n",
    "#- Binning\n",
    "#- Fixed width binning\n",
    "data['value_bin'] = pd.cut(data['column_to_bin'], bins=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "#- Qauntile based binning\n",
    "data['value_quantile_bin'] = pd.qcut(data['column_to_bin'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "#- Custom binning\n",
    "bins = []\n",
    "data['value_custom_bin'] = pd.cut(data['column_to_bin'], bins=bins, labels=[\"Low\", \"Medium\", \"High\"], right=False)\n",
    "\n",
    "#- Turn categorical variables into numerical variables\n",
    "#- Label encoding\n",
    "# Define a mapping of categories to numerical values\n",
    "checking_mapping = {'None': 0, 'little': 1, 'moderate': 2, 'rich': 3}\n",
    "# Map the categories to their numerical equivalents\n",
    "data['col_name'] = data['col_name_numb'].map(checking_mapping)\n",
    "#- One-hot encoding of categorical variables\n",
    "categorical_cols = []       # Add the columns of interest\n",
    "data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Outlier Treatment\n",
    "\n",
    "# for col, info in outlier_info.items():\n",
    "#     data = data.drop(index=info['outliers_index'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 3:</b> Exploratory Data Analysis (problem navigation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_time_series(data, time_col, target_col, freq='D', title=None):\n",
    "    \"\"\"\n",
    "    Visualize the target feature over time with flexible time aggregation, including hourly data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data containing time and target columns.\n",
    "        time_col (str): The column name representing time (must be in datetime format).\n",
    "        target_col (str): The column name representing the target variable to visualize.\n",
    "        freq (str): Frequency for resampling. Examples:\n",
    "            - 'H': Hourly\n",
    "            - 'D': Daily\n",
    "            - 'W': Weekly\n",
    "            - 'M': Monthly\n",
    "            - 'Q': Quarterly\n",
    "            - 'Y': Yearly\n",
    "        title (str): Optional title for the plot.\n",
    "    \n",
    "    Returns:\n",
    "        None: Displays a plot of the target feature over time.\n",
    "    \"\"\"\n",
    "    # Ensure the time column is in datetime format\n",
    "    data[time_col] = pd.to_datetime(data[time_col], errors='coerce')\n",
    "    \n",
    "    # Remove rows with missing values in the time or target columns\n",
    "    data = data.dropna(subset=[time_col, target_col])\n",
    "    \n",
    "    # Set the time column as the index\n",
    "    data = data.set_index(time_col)\n",
    "    \n",
    "    # Resample the data based on the specified frequency\n",
    "    resampled_data = data[target_col].resample(freq).mean()\n",
    "    \n",
    "    # Plot the resampled time series\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(resampled_data.index, resampled_data.values, marker='o', linestyle='-', markersize=4)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.title(title if title else f'{target_col} Over Time ({freq} Frequency)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Improve x-axis ticks for large datasets or sub-daily frequency\n",
    "    if freq in ['H', 'D', 'W']:\n",
    "        plt.gcf().autofmt_xdate()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hourly data\n",
    "visualize_time_series(data, date_col, target_col=target_col, freq='H')      # You'll have to specify the column that contains the time/date and the column that contains the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique categories\n",
    "unique_categories = data[category_col].unique()\n",
    "\n",
    "# Loop over each unique category\n",
    "for category in unique_categories:\n",
    "    print(f\"\\Display for {category_col}: {category}\")\n",
    "    \n",
    "    # Filter the data for the current category\n",
    "    category_data = data[data[category_col] == category].copy()\n",
    "    \n",
    "    visualize_time_series(category_data, date_col, target_col=target_col, freq='H')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display other timeframe, change the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_heatmap(data, time_col, value_col, x_freq, y_freq, agg_func='sum', cmap='coolwarm'):\n",
    "    \"\"\"\n",
    "    Generate a heatmap for time-based analysis with customizable resolutions (e.g., hour/day, week/day, month/week).\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data containing time and value columns.\n",
    "        time_col (str): The column representing time (must be in datetime format).\n",
    "        value_col (str): The column with values to aggregate (e.g., 'Entrées').\n",
    "        x_freq (str): Frequency for the x-axis. Options: 'hour', 'day', 'week', 'month', 'year'.\n",
    "        y_freq (str): Frequency for the y-axis. Options: 'hour', 'day', 'week', 'month', 'year'.\n",
    "        agg_func (str): Aggregation function to apply ('sum', 'mean', etc.).\n",
    "        cmap (str): Color map for the heatmap.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays a heatmap.\n",
    "    \"\"\"\n",
    "    # Ensure the time column is in datetime format\n",
    "    data[time_col] = pd.to_datetime(data[time_col], errors='coerce')\n",
    "    \n",
    "    # Extract the necessary time-based features\n",
    "    time_features = {\n",
    "        'hour': data[time_col].dt.hour,\n",
    "        'day': data[time_col].dt.day_name(),\n",
    "        'week': data[time_col].dt.isocalendar().week,\n",
    "        'month': data[time_col].dt.month_name(),\n",
    "        'year': data[time_col].dt.year\n",
    "    }\n",
    "    \n",
    "    # Ensure x_freq and y_freq are valid\n",
    "    if x_freq not in time_features or y_freq not in time_features:\n",
    "        raise ValueError(f\"Invalid frequency. Choose from {list(time_features.keys())}\")\n",
    "    \n",
    "    # Add the selected features to the DataFrame\n",
    "    data['x_feature'] = time_features[x_freq]\n",
    "    data['y_feature'] = time_features[y_freq]\n",
    "    \n",
    "    # Create a pivot table for the heatmap\n",
    "    pivot_table = data.pivot_table(index='y_feature', columns='x_feature', values=value_col, aggfunc=agg_func)\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.heatmap(pivot_table, cmap=cmap, linewidths=0.5, annot=True, fmt=\".0f\")\n",
    "    plt.title(f\"Heatmap of {value_col} by {y_freq.capitalize()} and {x_freq.capitalize()}\")\n",
    "    plt.xlabel(x_freq.capitalize())\n",
    "    plt.ylabel(y_freq.capitalize())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month/Year heatmap\n",
    "time_based_heatmap(data, \n",
    "                  time_col=date_col, \n",
    "                  value_col=target_col, \n",
    "                  x_freq='year', \n",
    "                  y_freq='month', \n",
    "                  agg_func='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique categories\n",
    "unique_categories = data[category_col].unique()\n",
    "\n",
    "# Loop over each unique category\n",
    "for category in unique_categories:\n",
    "    print(f\"\\Display for {category_col}: {category}\")\n",
    "    \n",
    "    # Filter the data for the current category\n",
    "    category_data = data[data[category_col] == category].copy()\n",
    "    \n",
    "    # Month/Year heatmap\n",
    "    time_based_heatmap(category_data, \n",
    "                  time_col=date_col, \n",
    "                  value_col=target_col, \n",
    "                  x_freq='year', \n",
    "                  y_freq='month', \n",
    "                  agg_func='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_univariate_analysis(df, col_name, visualize=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Perform univariate analysis on a numerical column with visualization\n",
    "    options using a Matplotlib native color palette focusing on red, yellow, and black.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing the data.\n",
    "    col_name (str): Name of the numerical column to analyze.\n",
    "    visualize (bool): Whether to visualize the analysis (default=True).\n",
    "    **kwargs: Additional keyword arguments to customize the visualisation colors.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: Descriptive statistics of the column.\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{col_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # 1. Descriptive Statistics\n",
    "    descriptive_stats = df[col_name].describe()\n",
    "    print(\"\\n\"*2, col_name, \"\\n\"*2, descriptive_stats)\n",
    "\n",
    "    if visualize:\n",
    "        # Custom color palette\n",
    "        custom_colors = ['red', 'yellow', 'black']  # Red, Yellow, Black\n",
    "        color = kwargs.get('hist_color', custom_colors[0])  # Use red as default\n",
    "        \n",
    "        # 2. Histogram with KDE\n",
    "        plt.figure(figsize=kwargs.get('figsize', (12, 6)))\n",
    "        sns.histplot(df[col_name], kde=True, bins=kwargs.get('bins', 30),\n",
    "                     color=color,\n",
    "                     kde_kws={'bw_adjust': kwargs.get('bw_adjust', 1)},\n",
    "                     line_kws={'color': custom_colors[2], 'lw': 2})  # Use black for KDE line\n",
    "        plt.title(f'Histogram and KDE of {col_name}')\n",
    "        plt.xlabel(col_name)\n",
    "        plt.ylabel('')\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5, color=custom_colors[1])  # Use yellow for grid lines\n",
    "        plt.show()\n",
    "\n",
    "        # 3. Boxplot\n",
    "        plt.figure(figsize=kwargs.get('figsize', (12, 6)))\n",
    "        sns.boxplot(x=df[col_name], color=kwargs.get('boxplot_color', custom_colors[1]))  # Use yellow for boxplot\n",
    "        plt.title(f'Boxplot of {col_name}')\n",
    "        plt.xlabel(col_name)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5, color=custom_colors[2])  # Use black for grid lines\n",
    "        plt.show()\n",
    "\n",
    "    return descriptive_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quant_col in []:        # Add in the brackets the name of the quantitative variables in your dataset that you want to visualize.\n",
    "    quant_univariate_analysis(data, quant_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qual_univariate_analysis(df, col_name, palette=\"viridis\", show_grid=True, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Performs and visualizes a univariate analysis for a qualitative (categorical) variable,\n",
    "    highlighting and annotating the most and least common categories.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The pandas DataFrame containing the data.\n",
    "    - col_name (str): The name of the column to analyze.\n",
    "    - palette (str, optional): Color palette for the plots. Defaults to 'viridis'.\n",
    "    - show_grid (bool, optional): Whether to show the grid in the bar plot. Defaults to True.\n",
    "    - figsize (tuple, optional): Figure size for the plots. Defaults to (12, 6).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Frequency Table\n",
    "    freq_table = df[col_name].value_counts()\n",
    "    # Percentage Table\n",
    "    percent_table = df[col_name].value_counts(normalize=True) * 100\n",
    "    combined_table = pd.DataFrame({'Frequency': freq_table, 'Percentage': percent_table})\n",
    "    print(\"\\n\"*2, col_name, \"\\n\"*2, combined_table)\n",
    "\n",
    "    # Identify most and least common categories\n",
    "    most_common = freq_table.idxmax()\n",
    "    least_common = freq_table.idxmin()\n",
    "\n",
    "    # Bar Plot with Highlighting\n",
    "    plt.figure(figsize=figsize)\n",
    "    barplot = sns.countplot(y=df[col_name], order=freq_table.index, palette=palette)\n",
    "    \n",
    "    # Highlighting\n",
    "    for patch in barplot.patches:\n",
    "        if patch.get_y() == freq_table.index.get_loc(most_common):\n",
    "            patch.set_facecolor('green')  # Highlight most common category in green\n",
    "        elif patch.get_y() == freq_table.index.get_loc(least_common):\n",
    "            patch.set_facecolor('red')  # Highlight least common category in red\n",
    "    \n",
    "    # Annotations\n",
    "    plt.title(f'Bar Plot of {col_name}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(col_name)\n",
    "    if show_grid:\n",
    "        plt.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Adding annotations for the most and least common categories\n",
    "    plt.text(freq_table.max(), freq_table.index.get_loc(most_common), 'Most common', fontsize=12, va='center')\n",
    "    plt.text(freq_table.min(), freq_table.index.get_loc(least_common), 'Least common', fontsize=12, va='center')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return combined_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qual_col in []:         # Add in the brackets the name of the qualitative variables in your dataset that you want to visualize.\n",
    "    qual_univariate_analysis(data, qual_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multivariate_time_series(data, time_col, target_col, condition_col, condition_func, freq='D', title=None):\n",
    "    \"\"\"\n",
    "    Visualize the target time series with highlighted points where another variable meets a certain condition.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The input data containing time, target, and condition columns.\n",
    "        time_col (str): The column name representing time (must be in datetime format).\n",
    "        target_col (str): The column name representing the target variable to visualize.\n",
    "        condition_col (str): The column name representing the variable to check the condition.\n",
    "        condition_func (function): A function that takes a pandas Series and returns a boolean Series indicating the condition.\n",
    "        freq (str): Frequency for resampling (e.g., 'H' for hourly, 'D' for daily, 'M' for monthly).\n",
    "        title (str): Optional title for the plot.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays a plot of the target feature over time with highlights.\n",
    "    \"\"\"\n",
    "    # Ensure the time column is in datetime format\n",
    "    data[time_col] = pd.to_datetime(data[time_col], errors='coerce')\n",
    "    \n",
    "    # Remove rows with missing values in the time or target columns\n",
    "    data = data.dropna(subset=[time_col, target_col, condition_col])\n",
    "    \n",
    "    # Set the time column as the index\n",
    "    data = data.set_index(time_col)\n",
    "    \n",
    "    # Resample the data based on the specified frequency\n",
    "    resampled_data = data[[target_col, condition_col]].resample(freq).mean()\n",
    "    \n",
    "    # Apply the condition function to determine where to highlight\n",
    "    condition_met = condition_func(resampled_data[condition_col])\n",
    "    \n",
    "    # Plot the target time series\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(resampled_data.index, resampled_data[target_col], label=target_col, color='blue', linestyle='-')\n",
    "    \n",
    "    # Highlight points where the condition is met\n",
    "    plt.scatter(resampled_data.index[condition_met], resampled_data[target_col][condition_met], \n",
    "                color='red', label=f'{condition_col} Condition Met', zorder=5)\n",
    "    \n",
    "    # Plot settings\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(target_col)\n",
    "    plt.title(title if title else f'{target_col} Over Time with {condition_col} Highlights')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a condition for highlighting using a condition function (condition_func), which should take a Pandas Series and return a boolean mask where the condition is met. For example, to highlight where a column temperature exceeds 30, use lambda x: x > 30. Call the function with the appropriate parameters and specify the frequency (freq) for resampling (H for hourly, D for daily, etc.). The function will plot the target variable over time and highlight points where the condition is true, making it easy to spot patterns, relations and anomalies in your time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Define the condition function: temperature > 30\n",
    "condition_func = lambda x: x > 30\n",
    "\n",
    "#- Visualize the time series with highlights\n",
    "visualize_multivariate_time_series(data, time_col=date_col, target_col=target_col, \n",
    "                                   condition_col=condition_col, condition_func=condition_func, freq='D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique categories\n",
    "unique_categories = data[category_col].unique()\n",
    "\n",
    "# Loop over each unique category\n",
    "for category in unique_categories:\n",
    "    print(f\"\\Display for {category_col}: {category}\")\n",
    "    \n",
    "    # Filter the data for the current category\n",
    "    category_data = data[data[category_col] == category].copy()\n",
    "    \n",
    "    #- Visualize the time series with highlights\n",
    "    visualize_multivariate_time_series(category_data, time_col=date_col, target_col=target_col, \n",
    "                                   condition_col=condition_col, condition_func=condition_func, freq='D')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Auto-correlation analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acf_pacf(series, lags=50, title_prefix=\"Time Series\"):\n",
    "    \"\"\"\n",
    "    Plot both ACF and PACF for a given time series.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The time series data.\n",
    "        lags (int): The number of lags to display on the ACF and PACF plots.\n",
    "        title_prefix (str): Prefix for the plot titles.\n",
    "        \n",
    "    Returns:\n",
    "        None: Displays the ACF and PACF plots.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot ACF\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plot_acf(series.dropna(), lags=lags, ax=plt.gca())\n",
    "    plt.title(f'{title_prefix} - Autocorrelation (ACF)')\n",
    "    \n",
    "    # Plot PACF\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plot_pacf(series.dropna(), lags=lags, ax=plt.gca())\n",
    "    plt.title(f'{title_prefix} - Partial Autocorrelation (PACF)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plot ACF and PACF for a time series\n",
    "plot_acf_pacf(data[target_col], lags=52, title_prefix=\"Mall Entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique categories\n",
    "unique_categories = data[category_col].unique()\n",
    "\n",
    "# Loop over each unique category\n",
    "for category in unique_categories:\n",
    "    print(f\"\\Display for {category_col}: {category}\")\n",
    "    \n",
    "    # Filter the data for the current category\n",
    "    category_data = data[data[category_col] == category].copy()\n",
    "    \n",
    "    # Example: Plot ACF and PACF for a time series\n",
    "    plot_acf_pacf(category_data[target_col], lags=52, title_prefix=\"Mall Entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series):\n",
    "    \"\"\"\n",
    "    Performs the Augmented Dickey-Fuller test on a given time series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        The time series to test.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing the Mall_ID, ADF statistic, p-value, and stationarity status.\n",
    "    \"\"\"\n",
    "    result = adfuller(series.dropna())\n",
    "    return {\n",
    "        \"ADF Statistic\": result[0],\n",
    "        \"p-value\": result[1],\n",
    "        \"Critical Values\": result[4],\n",
    "        \"Stationary\": result[1] < 0.05\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(data[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique categories\n",
    "unique_categories = data[category_col].unique()\n",
    "\n",
    "# Loop over each unique category\n",
    "for category in unique_categories:\n",
    "    print(f\"\\Display for {category_col}: {category}\")\n",
    "    \n",
    "    # Filter the data for the current category\n",
    "    category_data = data[data[category_col] == category].copy()\n",
    "    \n",
    "    adf_test(category_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Correlation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Calculate the correlation matrix\n",
    "correlation_cols = []       # Add in the brackets the name of the the columns you want to visualize for correlation. Make sure these columns are numeric. You can numerize the qualitative columns back in the preprocessing step.\n",
    "correlation = data[correlation_cols].corr(method='pearson')\n",
    "\n",
    "#- Visualize the correlation matrix\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.figure.set_size_inches(10, 10)\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(correlation, cmap=cmap, mask=mask, square=True, linewidths=.5, \n",
    "            annot=True, annot_kws={'size':14})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_corr = correlation[(correlation > 0.7) | (correlation < -0.7)]\n",
    "print(\"Strong correlations:\\n\", strong_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 4:</b> Feature Engineering\n",
    "*The Feature engineering phase involves transforming raw data into meaningful features that effectively represent the underlying problem.*  \n",
    "*<b>Feature Creation:</b>* Develop new features from the existing data to better capture the underlying patterns.  \n",
    "*<b>Feature Transformation:</b>* Apply transformations (e.g., scaling, encoding) to make the data suitable for modeling.  \n",
    "*<b>Feature Selection:</b>* Use statistical tests and selection algorithms to reduce dimensionality and focus on relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_feature_eng(df):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = do_feature_eng(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 5:</b> Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Train test split</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ccutoff dates for splitting\n",
    "tt_split_date = \"2024-10-01\"        # All data after this date will be used for test\n",
    "tv_split_date = \"2024-07-01\"        # All data between this date and tt_split_date will be used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "train_df = data[data[date_col] < tv_split_date]\n",
    "X_train, y_train = train_df.drop(columns=[target_col]), train_df[target_col]\n",
    "\n",
    "# Validation set\n",
    "val_df = data[(data[date_col] >= tv_split_date) & (data[date_col] < tt_split_date)]\n",
    "X_val, y_val = val_df.drop(columns=[target_col]), val_df[target_col]\n",
    "\n",
    "# Test set\n",
    "test_df = data[data[date_col] >= tt_split_date]\n",
    "X_test, y_test = test_df.drop(columns=[target_col]), test_df[target_col]\n",
    "\n",
    "full_train_df = data[data[date_col] < tt_split_date]\n",
    "X_full_train, y_full_train = full_train_df.drop(columns=[target_col]), full_train_df[target_col]\n",
    "\n",
    "print(f\"Train size: {X_train.shape}, Validation size: {X_val.shape}, Test size: {X_test.shape}, Full train size: {X_full_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 6:</b> Training & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Fitting a base model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Start a new MLflow run\n",
    "mlflow.set_experiment(\"baseline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    #- Define the baseline model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())  # Use Linear Regression as the model\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline to training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    \n",
    "    # Calculate evaluation metrics for regression\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"\\nMean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Log metrics with MLflow\n",
    "    mlflow.log_metric('mean_squared_error', mse)\n",
    "    mlflow.log_metric('mean_absolute_error', mae)\n",
    "    mlflow.log_metric('r2_score', r2)\n",
    "    \n",
    "    # Residuals plot\n",
    "    residuals = y_val - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.residplot(x=y_val, y=residuals, lowess=True, color=\"blue\")\n",
    "    plt.axhline(0, linestyle=\"--\", color=\"red\")\n",
    "    plt.title(\"Residuals Plot\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"residuals_plot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Log Residuals Plot as artifact\n",
    "    mlflow.log_artifact(\"residuals_plot.png\")\n",
    "    \n",
    "    # Predicted vs True values plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_val, y_pred, alpha=0.7, color=\"green\")\n",
    "    plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], '--r', linewidth=2)\n",
    "    plt.title(\"Predicted vs True Values\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"predicted_vs_true.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Log Predicted vs True Plot as artifact\n",
    "    mlflow.log_artifact(\"predicted_vs_true.png\")\n",
    "    \n",
    "    # Save the model and log it with MLflow\n",
    "    mlflow.sklearn.log_model(pipeline, \"baseline_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Fitting an hyper-optimized model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Start a new MLflow run\n",
    "mlflow.set_experiment(\"optimized_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Define regressors and their parameter grids\n",
    "regressors = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"Polynomial Regression\": Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(include_bias=False)),\n",
    "        (\"regressor\", LinearRegression())\n",
    "    ]),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(),\n",
    "    \"XGBoost Regressor\": XGBRegressor(),\n",
    "    \"Support Vector Regressor\": SVR(),\n",
    "    \"K-Nearest Neighbors Regressor\": KNeighborsRegressor(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(),\n",
    "    \"Gradient Boosting Regressor\": GradientBoostingRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_mlflow_tuner(trial):\n",
    "    \"\"\"\n",
    "    Function to optimize hyperparameters of a wide range of regressors using Optuna \n",
    "    and log detailed results with MLflow. Combines advanced feature engineering, \n",
    "    standardized scaling, and regression evaluation with detailed metrics and visualizations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    trial : optuna.trial.Trial\n",
    "        The trial object provided by Optuna to suggest hyperparameters and track the results.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The main evaluation metric (Mean Squared Error) of the model on the validation set.\n",
    "    \"\"\"\n",
    "    global best_model\n",
    "\n",
    "    # Start an MLflow run for this trial\n",
    "    with mlflow.start_run():\n",
    "        # Select a regressor\n",
    "        model_name = trial.suggest_categorical(\"model\", list(regressors.keys()))\n",
    "        regressor = regressors[model_name]\n",
    "\n",
    "        # Suggest hyperparameters specific to the selected regressor\n",
    "        if model_name == \"Linear Regression\":\n",
    "            fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "            regressor.set_params(fit_intercept=fit_intercept)\n",
    "\n",
    "        elif model_name == \"Ridge Regression\":\n",
    "            alpha = trial.suggest_loguniform(\"ridge_alpha\", 0.001, 100)\n",
    "            regressor.set_params(alpha=alpha)\n",
    "\n",
    "        elif model_name == \"Lasso Regression\":\n",
    "            alpha = trial.suggest_loguniform(\"lasso_alpha\", 0.001, 100)\n",
    "            regressor.set_params(alpha=alpha)\n",
    "\n",
    "        elif model_name == \"ElasticNet\":\n",
    "            alpha = trial.suggest_loguniform(\"elasticnet_alpha\", 0.001, 100)\n",
    "            l1_ratio = trial.suggest_uniform(\"elasticnet_l1_ratio\", 0.1, 1.0)\n",
    "            regressor.set_params(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "        elif model_name == \"Polynomial Regression\":\n",
    "            degree = trial.suggest_int(\"polynomial_degree\", 2, 5)\n",
    "            regressor.set_params(poly_features__degree=degree)\n",
    "\n",
    "        elif model_name == \"Random Forest Regressor\":\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "            min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
    "            regressor.set_params(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "        elif model_name == \"XGBoost Regressor\":\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "            learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.5)\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
    "            subsample = trial.suggest_uniform(\"subsample\", 0.5, 1.0)\n",
    "            colsample_bytree = trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0)\n",
    "            regressor.set_params(\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                n_estimators=n_estimators,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree\n",
    "            )\n",
    "\n",
    "        elif model_name == \"Support Vector Regressor\":\n",
    "            kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"])\n",
    "            C = trial.suggest_loguniform(\"C\", 0.1, 100)\n",
    "            epsilon = trial.suggest_loguniform(\"epsilon\", 0.01, 1.0)\n",
    "            regressor.set_params(kernel=kernel, C=C, epsilon=epsilon)\n",
    "\n",
    "        elif model_name == \"K-Nearest Neighbors Regressor\":\n",
    "            n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 50)\n",
    "            weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "            algorithm = trial.suggest_categorical(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"])\n",
    "            regressor.set_params(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm)\n",
    "\n",
    "        elif model_name == \"Decision Tree Regressor\":\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "            min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
    "            regressor.set_params(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "        elif model_name == \"Gradient Boosting Regressor\":\n",
    "            learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.5)\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "            regressor.set_params(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth)\n",
    "\n",
    "        # Build the pipeline\n",
    "        pipeline = Pipeline([\n",
    "            (\"feature_engineering\", FunctionTransformer(do_feature_eng, validate=False)),  # Replace with your feature engineering function\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"regressor\", regressor)\n",
    "        ])\n",
    "\n",
    "        # Train the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "\n",
    "        # Calculate evaluation metrics for regression\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "        # Log metrics in MLflow\n",
    "        mlflow.log_metric(\"mean_squared_error\", mse)\n",
    "        mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "        mlflow.log_metric(\"r2_score\", r2)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "\n",
    "        # Log hyperparameters\n",
    "        params = regressor.get_params()\n",
    "        for param_name, param_value in params.items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "\n",
    "        # Plot predicted vs actual values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_val, y_pred, alpha=0.6, color=\"blue\")\n",
    "        plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], '--r', linewidth=2)\n",
    "        plt.title(f\"Predicted vs Actual Values - {model_name}\")\n",
    "        plt.xlabel(\"Actual Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"predicted_vs_actual.png\")\n",
    "        mlflow.log_artifact(\"predicted_vs_actual.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save the model if it's the best one found so far\n",
    "        if trial.number == 0 or mse < study.best_value:\n",
    "            best_model = pipeline\n",
    "\n",
    "        return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(optuna_mlflow_tuner, n_trials=50)\n",
    "\n",
    "print(\"\\nBest hyperparameters found:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Evaluation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Évalue un modèle de machine learning en calculant plusieurs scores d'erreur.\n",
    "    \n",
    "    Paramètres :\n",
    "    - model : modèle entraîné (XGBoost, RandomForest, etc.)\n",
    "    - X_test : Features du jeu de test\n",
    "    - y_test : Valeurs réelles des entrées\n",
    "    \n",
    "    Retourne :\n",
    "    - Un dictionnaire avec les scores MAE, RMSE et R² + un commentaire d'évaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prédiction sur les données de test\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calcul des scores\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "\n",
    "    # Retourner les résultats\n",
    "    results = {\n",
    "        \"MAE\": round(mae, 2),\n",
    "        \"RMSE\": round(rmse, 2),\n",
    "        \"R²\": round(r2, 2),\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cross_validation(model, X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Effectue une validation croisée adaptée aux séries temporelles avec TimeSeriesSplit.\n",
    "    \n",
    "    Paramètres :\n",
    "    - model : modèle de ML (XGBoost, RandomForest, etc.)\n",
    "    - X : Features\n",
    "    - y : Cible (nombre d'entrées)\n",
    "    - n_splits : Nombre de splits pour la validation\n",
    "    \n",
    "    Retourne :\n",
    "    - Moyenne des scores MAE, RMSE et R² sur les splits\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    mae_scores, rmse_scores, r2_scores = [], [], []\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        # Séparer les données en train/test\n",
    "        X_retrain, X_retest = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_retrain, y_retest = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Entraîner le modèle\n",
    "        model.fit(X_retrain, y_retrain)\n",
    "        \n",
    "        # Prédire sur les données de test\n",
    "        y_pred = model.predict(X_retest)\n",
    "        \n",
    "        # Calculer les scores\n",
    "        mae_scores.append(mean_absolute_error(y_retest, y_pred))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_retest, y_pred)))\n",
    "        r2_scores.append(r2_score(y_retest, y_pred))\n",
    "\n",
    "    # Moyenne des scores\n",
    "    results = {\n",
    "        \"MAE moyen\": round(np.mean(mae_scores), 2),\n",
    "        \"RMSE moyen\": round(np.mean(rmse_scores), 2),\n",
    "        \"R² moyen\": round(np.mean(r2_scores), 2),\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_cross_validation(best_model, X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 7:</b> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Retrain the model on the whole dataset \n",
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_to_predict = [[]]\n",
    "predictions = best_model.predict(X_to_predict)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_2 = \".csv\"\n",
    "data_2 = pd.read_csv(data_path_2)\n",
    "raw_data_2 = data_2.copy()\n",
    "data_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_inspection(data_2, visualize_nulls=True, detailed_summary=True, check_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = [\"feature1\", \"feature2\", \"feature3\"]  # Replace with actual feature names\n",
    "X_to_predict = data_2[X_features]\n",
    "data_2 = data_cleaning(X_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_to_predict)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the original data\n",
    "results_df = raw_data_2.copy()  # Copy the original data for context\n",
    "results_df[\"Predicted Values\"] = predictions  # Add predicted values as a new column\n",
    "\n",
    "results_file_path = \"predictions_results.csv\"\n",
    "results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "print(f\"Results exported successfully to {results_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
