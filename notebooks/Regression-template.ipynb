{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>Regression template</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 0:</b> Context\n",
    "*Explain the problem you're trying to solve, the nature of the dataset, the goal of the model, and the impact of the solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 1:</b> Imports and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Libraries</b>\n",
    "*Import all necessary Python libraries (e.g., numpy, pandas, scikit-learn, tensorflow/pytorch for deep learning).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Workflow\n",
    "from sklearn.pipeline import Pipeline\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "#- Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#- Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#- Preprocessing for model fitting\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#- ML models (for regression)\n",
    "import optuna\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#- Model assessment for regression\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Dataset</b>\n",
    "*If you have several data sets, you need to duplicate the cells below. If your dataset is in a different format, create a new cell and write the code to import it. Don't forget Chat GPT and Google are your friends. ;-)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If your data is in a CSV file use the cell below to import it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \".csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "raw_data = data.copy()\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If your data is in an excel file use the cell below to import it instead.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \".xlsx\"\n",
    "# data = pd.read_excel(data_path)\n",
    "# data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Configurations</b>\n",
    "*Set any global configurations such as random seed for reproducibility, backend settings for computation libraries, and any project-specific parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "#- Show all the column\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#- Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"test_mlflow_setup\")      # This is just to test if the mlflow server is working\n",
    "try:\n",
    "    mlflow.log_param(\"test_param\", \"test_value\")\n",
    "    print(\"MLflow is configured correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"MLflow configuration failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 2:</b> Data inspection and cleanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Inspection</b>\n",
    "*The goal of data inspection is to understand the dataset's characteristics, quality, and any potential issues that need addressing before analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_inspection(df, visualize_nulls=True, detailed_summary=True, check_duplicates=True):\n",
    "    \"\"\"\n",
    "    Perform a basic inspection of a pandas DataFrame including size, data types,\n",
    "    null values, a heatmap of missing values, duplicate rows, and a summary table with optional\n",
    "    detailed analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - visualize_nulls: Bool, if True, visualizes null values using a heatmap.\n",
    "    - detailed_summary: Bool, if True, includes detailed analysis in the summary table.\n",
    "    - check_duplicates: Bool, if True, checks and reports the number of duplicate rows.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing a summary of the inspection.\n",
    "    \"\"\"\n",
    "    print(f'Number of rows: {df.shape[0]}\\nNumber of columns: {df.shape[1]}\\n')\n",
    "    print('Data types:\\n', df.dtypes)\n",
    "    \n",
    "    null_counts = df.isnull().sum()\n",
    "    print('\\nColumns with null values:\\n', null_counts[null_counts > 0])\n",
    "\n",
    "    if check_duplicates:\n",
    "        duplicate_rows = df.duplicated().sum()\n",
    "        print(f'\\nNumber of duplicate rows: {duplicate_rows}\\n')\n",
    "\n",
    "    if visualize_nulls:\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        sns.heatmap(df.isna(), cbar=True, cmap='viridis')\n",
    "        plt.title('Heatmap of Missing Values')\n",
    "        plt.show()\n",
    "\n",
    "    summary_table = pd.DataFrame({\n",
    "        \"Unique_values\": df.nunique(),\n",
    "        \"Data_type\": df.dtypes,\n",
    "        \"Null_count\": df.isnull().sum(),\n",
    "        \"Null_percentage\": (df.isnull().sum() / df.shape[0] * 100).round(2)\n",
    "    })\n",
    "\n",
    "    if detailed_summary:\n",
    "        if 'object' in df.dtypes.values:\n",
    "            summary_table['Most_common'] = df.apply(lambda x: x.value_counts().idxmax() if x.dtypes == 'object' else 'unknown')\n",
    "        for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "            summary_table.loc[col, 'Mean'] = df[col].mean()\n",
    "            summary_table.loc[col, 'Std'] = df[col].std()\n",
    "            summary_table.loc[col, 'Min'] = df[col].min()\n",
    "            summary_table.loc[col, 'Max'] = df[col].max()\n",
    "\n",
    "    return summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the following cell, modify the parameters according to the particularities of your database. For example, if you have intentional duplicates, it is not necessary to check them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_inspection(data, visualize_nulls=True, detailed_summary=True, check_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(df, plot=True):\n",
    "    \"\"\"\n",
    "    Identify outliers in a pandas DataFrame using Z-score (values more than 3 standard deviations from \n",
    "    the mean) and Interquartile Range (IQR) methods (values below Q1 - 1.92*IQR or above Q3 + 1.92*IQR).\n",
    "    Outliers identified by both methods are combined to provide a comprehensive overview of outliers \n",
    "    in each numeric feature.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): A pandas DataFrame containing the data to be analyzed. Only numeric\n",
    "      columns will be considered for outlier detection.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where each key is the name of a numeric feature in the DataFrame. Each value\n",
    "      is another dictionary containing two keys: 'num_outliers', which is the number of unique outliers\n",
    "      identified in the feature, and 'outliers_index', an array of indices of these outliers.\n",
    "    \"\"\"\n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for column in df.select_dtypes(include=np.number).columns:  # Focus on numeric columns\n",
    "        # Calculate Z-scores\n",
    "        z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "        z_outliers = np.where(z_scores > 3)[0]\n",
    "        \n",
    "        # Calculate IQR\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        iqr_outliers = df[(df[column] < (Q1 - 1.96 * IQR)) | (df[column] > (Q3 + 1.96 * IQR))].index\n",
    "        \n",
    "        # Combine unique outliers from both methods\n",
    "        combined_outliers = np.union1d(z_outliers, iqr_outliers)\n",
    "        \n",
    "        # Optionally plot\n",
    "        if plot:\n",
    "            print(column)\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.boxplot(x=df[column])\n",
    "            plt.title(f'Boxplot of {column} (Outliers highlighted)')\n",
    "            plt.show()\n",
    "\n",
    "        # Summary\n",
    "        outlier_summary[column] = {\n",
    "            'num_outliers': len(combined_outliers),\n",
    "            'outliers_index': combined_outliers\n",
    "        }\n",
    "    \n",
    "    return outlier_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This function does not modify the original DataFrame. I recommend you to inspect the identified outliers and decide on appropriate handling methods such as removal, replacement, or keeping them as is, depending on the analysis requirements and domain knowledge.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_info = identify_outliers(data)\n",
    "print(outlier_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Cleaning</b>\n",
    "*Following the inspection, data cleaning aims to rectify issues identified, improving the dataset's quality and making it suitable for further analysis and modeling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Feature Selection\n",
    "\n",
    "# data.drop(column=[], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Remove duplicates\n",
    "\n",
    "# data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data):\n",
    "    #- Handle missing values\n",
    "    data['numerical_column'].fillna(data['numerical_column'].mean(), inplace=True)\n",
    "    data['categorical_column'].fillna(data['categorical_column'].mode()[0], inplace=True)\n",
    "    \n",
    "    #- Correct data type\n",
    "    data['numeric_column_as_string'] = pd.to_numeric(data['numeric_column_as_string'], errors='coerce')\n",
    "    data['date_column'] = pd.to_datetime(data['date_column'])\n",
    "\n",
    "    #- Binning\n",
    "    #- Fixed width binning\n",
    "    data['value_bin'] = pd.cut(data['column_to_bin'], bins=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "    #- Qauntile based binning\n",
    "    data['value_quantile_bin'] = pd.qcut(data['column_to_bin'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "    #- Custom binning\n",
    "    bins = []\n",
    "    data['value_custom_bin'] = pd.cut(data['column_to_bin'], bins=bins, labels=[\"Low\", \"Medium\", \"High\"], right=False)\n",
    "\n",
    "    #- Turn categorical variables into numerical variables\n",
    "\n",
    "    #- Label encoding\n",
    "    # Define a mapping of categories to numerical values\n",
    "    checking_mapping = {'None': 0, 'little': 1, 'moderate': 2, 'rich': 3}\n",
    "    # Map the categories to their numerical equivalents\n",
    "    data['col_name'] = data['col_name_numb'].map(checking_mapping)\n",
    "\n",
    "    #- One-hot encoding of categorical variables\n",
    "    categorical_cols = []       # Add the columns of interest\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cleaning(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>When deciding between one-hot encoding and simple encoding (label encoding) for categorical data, consider the nature of your categories:\n",
    "\n",
    "- One-Hot Encoding: Use this when your categorical variable does not have a meaningful order or hierarchy. One-hot encoding creates a new binary column for each category, which is ideal for nominal data (e.g., color, city names). This approach prevents the model from assuming a natural ordering between categories, which is helpful to avoid misleading interpretations.\n",
    "\n",
    "- Label Encoding: Use this when your categorical variable has a meaningful order or ranking (ordinal data). Label encoding assigns a unique integer to each category, preserving the order. However, be cautious, as this can introduce unintended ordinal relationships that may not exist in the data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also don't apply the cleaning of the target feature in the data_cleaning function as we will apply this function to the prediction dataset (which won't have the target column) too. Do the transformation for the target column in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Handle missing values\n",
    "data['numerical_column'].fillna(data['numerical_column'].mean(), inplace=True)\n",
    "\n",
    "#- Correct data type\n",
    "data['numeric_column_as_string'] = pd.to_numeric(data['numeric_column_as_string'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Outlier Treatment\n",
    "\n",
    "# for col, info in outlier_info.items():\n",
    "#     data = data.drop(index=info['outliers_index'], inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 3:</b> Exploratory Data Analysis (problem navigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Univariate analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_univariate_analysis(df, col_name, visualize=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Perform univariate analysis on a numerical column with visualization\n",
    "    options using a Matplotlib native color palette focusing on red, yellow, and black.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing the data.\n",
    "    col_name (str): Name of the numerical column to analyze.\n",
    "    visualize (bool): Whether to visualize the analysis (default=True).\n",
    "    **kwargs: Additional keyword arguments to customize the visualisation colors.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: Descriptive statistics of the column.\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{col_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # 1. Descriptive Statistics\n",
    "    descriptive_stats = df[col_name].describe()\n",
    "    print(\"\\n\"*2, col_name, \"\\n\"*2, descriptive_stats)\n",
    "\n",
    "    if visualize:\n",
    "        # Custom color palette\n",
    "        custom_colors = ['red', 'yellow', 'black']  # Red, Yellow, Black\n",
    "        color = kwargs.get('hist_color', custom_colors[0])  # Use red as default\n",
    "        \n",
    "        # 2. Histogram with KDE\n",
    "        plt.figure(figsize=kwargs.get('figsize', (12, 6)))\n",
    "        sns.histplot(df[col_name], kde=True, bins=kwargs.get('bins', 30),\n",
    "                     color=color,\n",
    "                     kde_kws={'bw_adjust': kwargs.get('bw_adjust', 1)},\n",
    "                     line_kws={'color': custom_colors[2], 'lw': 2})  # Use black for KDE line\n",
    "        plt.title(f'Histogram and KDE of {col_name}')\n",
    "        plt.xlabel(col_name)\n",
    "        plt.ylabel('')\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5, color=custom_colors[1])  # Use yellow for grid lines\n",
    "        plt.show()\n",
    "\n",
    "        # 3. Boxplot\n",
    "        plt.figure(figsize=kwargs.get('figsize', (12, 6)))\n",
    "        sns.boxplot(x=df[col_name], color=kwargs.get('boxplot_color', custom_colors[1]))  # Use yellow for boxplot\n",
    "        plt.title(f'Boxplot of {col_name}')\n",
    "        plt.xlabel(col_name)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5, color=custom_colors[2])  # Use black for grid lines\n",
    "        plt.show()\n",
    "\n",
    "    return descriptive_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quant_col in []:        # Add in the brackets the name of the quantitative variables in your dataset that you want to visualize.\n",
    "    quant_univariate_analysis(data, quant_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qual_univariate_analysis(df, col_name, palette=\"viridis\", show_grid=True, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Performs and visualizes a univariate analysis for a qualitative (categorical) variable,\n",
    "    highlighting and annotating the most and least common categories.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The pandas DataFrame containing the data.\n",
    "    - col_name (str): The name of the column to analyze.\n",
    "    - palette (str, optional): Color palette for the plots. Defaults to 'viridis'.\n",
    "    - show_grid (bool, optional): Whether to show the grid in the bar plot. Defaults to True.\n",
    "    - figsize (tuple, optional): Figure size for the plots. Defaults to (12, 6).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Frequency Table\n",
    "    freq_table = df[col_name].value_counts()\n",
    "    # Percentage Table\n",
    "    percent_table = df[col_name].value_counts(normalize=True) * 100\n",
    "    combined_table = pd.DataFrame({'Frequency': freq_table, 'Percentage': percent_table})\n",
    "    print(\"\\n\"*2, col_name, \"\\n\"*2, combined_table)\n",
    "\n",
    "    # Identify most and least common categories\n",
    "    most_common = freq_table.idxmax()\n",
    "    least_common = freq_table.idxmin()\n",
    "\n",
    "    # Bar Plot with Highlighting\n",
    "    plt.figure(figsize=figsize)\n",
    "    barplot = sns.countplot(y=df[col_name], order=freq_table.index, palette=palette)\n",
    "    \n",
    "    # Highlighting\n",
    "    for patch in barplot.patches:\n",
    "        if patch.get_y() == freq_table.index.get_loc(most_common):\n",
    "            patch.set_facecolor('green')  # Highlight most common category in green\n",
    "        elif patch.get_y() == freq_table.index.get_loc(least_common):\n",
    "            patch.set_facecolor('red')  # Highlight least common category in red\n",
    "    \n",
    "    # Annotations\n",
    "    plt.title(f'Bar Plot of {col_name}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(col_name)\n",
    "    if show_grid:\n",
    "        plt.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Adding annotations for the most and least common categories\n",
    "    plt.text(freq_table.max(), freq_table.index.get_loc(most_common), 'Most common', fontsize=12, va='center')\n",
    "    plt.text(freq_table.min(), freq_table.index.get_loc(least_common), 'Least common', fontsize=12, va='center')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return combined_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qual_col in []:         # Add in the brackets the name of the qualitative variables in your dataset that you want to visualize.\n",
    "    qual_univariate_analysis(data, qual_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Bivariate analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Calculate the correlation matrix\n",
    "correlation_cols = []       # Add in the brackets the name of the the columns you want to visualize for correlation. Make sure these columns are numeric. You can numerize the qualitative columns back in the preprocessing step.\n",
    "correlation = data[correlation_cols].corr(method='pearson')\n",
    "\n",
    "#- Visualize the correlation matrix\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.figure.set_size_inches(10, 10)\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(correlation, cmap=cmap, mask=mask, square=True, linewidths=.5, \n",
    "            annot=True, annot_kws={'size':14})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_corr = correlation[(correlation > 0.7) | (correlation < -0.7)]\n",
    "print(\"Strong correlations:\\n\", strong_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 4:</b> Feature Engineering\n",
    "*The Feature engineering phase involves transforming raw data into meaningful features that effectively represent the underlying problem.*  \n",
    "*<b>Feature Creation:</b>* Develop new features from the existing data to better capture the underlying patterns.  \n",
    "*<b>Feature Transformation:</b>* Apply transformations (e.g., scaling, encoding) to make the data suitable for modeling.  \n",
    "*<b>Feature Selection:</b>* Use statistical tests and selection algorithms to reduce dimensionality and focus on relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_feature_eng(df):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = do_feature_eng(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 5:</b> Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Define Features and Target</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Features:\n",
    "features_cols = []\n",
    "X = data[features_cols]\n",
    "\n",
    "#- Target:\n",
    "y = data[target_name]       # Replace target_name with the name of your target variable in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Train validation and Test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_X_train, X_test, pre_y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(pre_X_train, pre_y_train, test_size=0.25, random_state=42, stratify=pre_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the X training database: \", X_train.shape)\n",
    "print(\"Size of the X validation database: \", X_val.shape)\n",
    "print(\"Size of the X testing database: \", X_test.shape)\n",
    "print(\"Size of the y training database: \", y_train.shape)\n",
    "print(\"Size of the y validation database: \", y_val.shape)\n",
    "print(\"Size of the y testing database: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 6:</b> Training & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Fitting a base model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Start a new MLflow run\n",
    "mlflow.set_experiment(\"baseline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    #- Define the baseline model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())  # Use Linear Regression as the model\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline to training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    \n",
    "    # Calculate evaluation metrics for regression\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"\\nMean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Log metrics with MLflow\n",
    "    mlflow.log_metric('mean_squared_error', mse)\n",
    "    mlflow.log_metric('mean_absolute_error', mae)\n",
    "    mlflow.log_metric('r2_score', r2)\n",
    "    \n",
    "    # Residuals plot\n",
    "    residuals = y_val - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.residplot(x=y_val, y=residuals, lowess=True, color=\"blue\")\n",
    "    plt.axhline(0, linestyle=\"--\", color=\"red\")\n",
    "    plt.title(\"Residuals Plot\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"residuals_plot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Log Residuals Plot as artifact\n",
    "    mlflow.log_artifact(\"residuals_plot.png\")\n",
    "    \n",
    "    # Predicted vs True values plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_val, y_pred, alpha=0.7, color=\"green\")\n",
    "    plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], '--r', linewidth=2)\n",
    "    plt.title(\"Predicted vs True Values\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"predicted_vs_true.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Log Predicted vs True Plot as artifact\n",
    "    mlflow.log_artifact(\"predicted_vs_true.png\")\n",
    "    \n",
    "    # Save the model and log it with MLflow\n",
    "    mlflow.sklearn.log_model(pipeline, \"baseline_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Fitting an hyper-optimized model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Start a new MLflow run\n",
    "mlflow.set_experiment(\"optimized_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Define regressors and their parameter grids\n",
    "regressors = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"Polynomial Regression\": Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(include_bias=False)),\n",
    "        (\"regressor\", LinearRegression())\n",
    "    ]),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(),\n",
    "    \"XGBoost Regressor\": xgb.XGBRegressor(),\n",
    "    \"Support Vector Regressor\": SVR(),\n",
    "    \"K-Nearest Neighbors Regressor\": KNeighborsRegressor(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(),\n",
    "    \"Gradient Boosting Regressor\": GradientBoostingRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_mlflow_tuner(trial):\n",
    "    \"\"\"\n",
    "    Function to optimize hyperparameters of a wide range of regressors using Optuna \n",
    "    and log detailed results with MLflow. Combines advanced feature engineering, \n",
    "    standardized scaling, and regression evaluation with detailed metrics and visualizations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    trial : optuna.trial.Trial\n",
    "        The trial object provided by Optuna to suggest hyperparameters and track the results.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The main evaluation metric (Mean Squared Error) of the model on the validation set.\n",
    "    \"\"\"\n",
    "    global best_model\n",
    "\n",
    "    # Start an MLflow run for this trial\n",
    "    with mlflow.start_run():\n",
    "        # Select a regressor\n",
    "        model_name = trial.suggest_categorical(\"model\", list(regressors.keys()))\n",
    "        regressor = regressors[model_name]\n",
    "\n",
    "        # Suggest hyperparameters specific to the selected regressor\n",
    "        if model_name == \"Linear Regression\":\n",
    "            fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "            regressor.set_params(fit_intercept=fit_intercept)\n",
    "\n",
    "        elif model_name == \"Ridge Regression\":\n",
    "            alpha = trial.suggest_loguniform(\"ridge_alpha\", 0.001, 100)\n",
    "            regressor.set_params(alpha=alpha)\n",
    "\n",
    "        elif model_name == \"Lasso Regression\":\n",
    "            alpha = trial.suggest_loguniform(\"lasso_alpha\", 0.001, 100)\n",
    "            regressor.set_params(alpha=alpha)\n",
    "\n",
    "        elif model_name == \"ElasticNet\":\n",
    "            alpha = trial.suggest_loguniform(\"elasticnet_alpha\", 0.001, 100)\n",
    "            l1_ratio = trial.suggest_uniform(\"elasticnet_l1_ratio\", 0.1, 1.0)\n",
    "            regressor.set_params(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "        elif model_name == \"Polynomial Regression\":\n",
    "            degree = trial.suggest_int(\"polynomial_degree\", 2, 5)\n",
    "            regressor.set_params(poly_features__degree=degree)\n",
    "\n",
    "        elif model_name == \"Random Forest Regressor\":\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "            min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
    "            regressor.set_params(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "        elif model_name == \"XGBoost Regressor\":\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "            learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.5)\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
    "            subsample = trial.suggest_uniform(\"subsample\", 0.5, 1.0)\n",
    "            colsample_bytree = trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0)\n",
    "            regressor.set_params(\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                n_estimators=n_estimators,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree\n",
    "            )\n",
    "\n",
    "        elif model_name == \"Support Vector Regressor\":\n",
    "            kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"])\n",
    "            C = trial.suggest_loguniform(\"C\", 0.1, 100)\n",
    "            epsilon = trial.suggest_loguniform(\"epsilon\", 0.01, 1.0)\n",
    "            regressor.set_params(kernel=kernel, C=C, epsilon=epsilon)\n",
    "\n",
    "        elif model_name == \"K-Nearest Neighbors Regressor\":\n",
    "            n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 50)\n",
    "            weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "            algorithm = trial.suggest_categorical(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"])\n",
    "            regressor.set_params(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm)\n",
    "\n",
    "        elif model_name == \"Decision Tree Regressor\":\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "            min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
    "            regressor.set_params(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "        elif model_name == \"Gradient Boosting Regressor\":\n",
    "            learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.01, 0.5)\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "            regressor.set_params(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth)\n",
    "\n",
    "        # Build the pipeline\n",
    "        pipeline = Pipeline([\n",
    "            (\"feature_engineering\", FunctionTransformer(do_feature_eng, validate=False)),  # Replace with your feature engineering function\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"regressor\", regressor)\n",
    "        ])\n",
    "\n",
    "        # Train the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "\n",
    "        # Calculate evaluation metrics for regression\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "        # Log metrics in MLflow\n",
    "        mlflow.log_metric(\"mean_squared_error\", mse)\n",
    "        mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "        mlflow.log_metric(\"r2_score\", r2)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "\n",
    "        # Log hyperparameters\n",
    "        params = regressor.get_params()\n",
    "        for param_name, param_value in params.items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "\n",
    "        # Plot predicted vs actual values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_val, y_pred, alpha=0.6, color=\"blue\")\n",
    "        plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], '--r', linewidth=2)\n",
    "        plt.title(f\"Predicted vs Actual Values - {model_name}\")\n",
    "        plt.xlabel(\"Actual Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"predicted_vs_actual.png\")\n",
    "        mlflow.log_artifact(\"predicted_vs_actual.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save the model if it's the best one found so far\n",
    "        if trial.number == 0 or mse < study.best_value:\n",
    "            best_model = pipeline\n",
    "\n",
    "        return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(optuna_mlflow_tuner, n_trials=50)\n",
    "\n",
    "print(\"\\nBest hyperparameters found:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Testing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(pre_X_train, pre_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate a trained regression model on the test set and log metrics and visualizations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The trained machine learning model (pipeline or standalone model).\n",
    "    X_test : array-like\n",
    "        The test set features.\n",
    "    y_test : array-like\n",
    "        The true labels of the test set.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing evaluation metrics (MSE, MAE, R² Score).\n",
    "    \"\"\"\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "    # Residuals plot\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.residplot(x=y_test, y=residuals, lowess=True, color=\"blue\")\n",
    "    plt.axhline(0, linestyle=\"--\", color=\"red\")\n",
    "    plt.title(\"Residuals Plot\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Predicted vs True Values plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7, color=\"green\")\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)\n",
    "    plt.title(\"Predicted vs True Values\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Return the metrics as a dictionary\n",
    "    return {\n",
    "        \"mean_squared_error\": mse,\n",
    "        \"mean_absolute_error\": mae,\n",
    "        \"r2_score\": r2\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_on_test(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model, X, y, cv=5, scoring=\"neg_mean_squared_error\"):\n",
    "    \"\"\"\n",
    "    Perform cross-validation on a regression model and report averaged performance metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The trained machine learning model (pipeline or standalone model).\n",
    "    X : array-like\n",
    "        The features of the dataset.\n",
    "    y : array-like\n",
    "        The target values of the dataset.\n",
    "    cv : int, optional\n",
    "        Number of folds for cross-validation. Default is 5.\n",
    "    scoring : str, optional\n",
    "        The scoring metric to use for cross-validation. Default is \"neg_mean_squared_error\".\n",
    "        Common options: \"r2\", \"neg_mean_squared_error\", \"neg_mean_absolute_error\".\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary with cross-validated metrics.\n",
    "    \"\"\"\n",
    "    # Define cross-validation strategy\n",
    "    kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    print(f\"Performing {cv}-fold cross-validation...\")\n",
    "    scores = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "    # Process and return results\n",
    "    if scoring == \"neg_mean_squared_error\":\n",
    "        scores = -scores  # Convert to positive MSE values\n",
    "        metric_name = \"Mean Squared Error (MSE)\"\n",
    "    elif scoring == \"neg_mean_absolute_error\":\n",
    "        scores = -scores  # Convert to positive MAE values\n",
    "        metric_name = \"Mean Absolute Error (MAE)\"\n",
    "    elif scoring == \"r2\":\n",
    "        metric_name = \"R² Score\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported scoring metric. Use 'r2', 'neg_mean_squared_error', or 'neg_mean_absolute_error'.\")\n",
    "\n",
    "    avg_score = scores.mean()\n",
    "    std_score = scores.std()\n",
    "\n",
    "    print(f\"{metric_name}: {avg_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"metric_name\": metric_name,\n",
    "        \"mean\": avg_score,\n",
    "        \"std\": std_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with full training data\n",
    "cross_val_results = cross_validate_model(best_model, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 7:</b> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Retrain the model on the whole dataset \n",
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_to_predict = [[]]\n",
    "predictions = best_model.predict(X_to_predict)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_2 = \".csv\"\n",
    "data_2 = pd.read_csv(data_path_2)\n",
    "raw_data_2 = data_2.copy()\n",
    "data_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_inspection(data_2, visualize_nulls=True, detailed_summary=True, check_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = [\"feature1\", \"feature2\", \"feature3\"]  # Replace with actual feature names\n",
    "X_to_predict = data_2[X_features]\n",
    "data_2 = data_cleaning(X_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(X_to_predict)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the original data\n",
    "results_df = raw_data_2.copy()  # Copy the original data for context\n",
    "results_df[\"Predicted Values\"] = predictions  # Add predicted values as a new column\n",
    "\n",
    "results_file_path = \"predictions_results.csv\"\n",
    "results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "print(f\"Results exported successfully to {results_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
