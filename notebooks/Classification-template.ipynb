{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><b>Classification template</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 0:</b> Context\n",
    "*Explain the problem you're trying to solve, the nature of the dataset, the goal of the model, and the impact of the solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 1:</b> Imports and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Libraries</b>\n",
    "*Import all necessary Python libraries (e.g., numpy, pandas, scikit-learn, tensorflow/pytorch for deep learning).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Workflow\n",
    "from sklearn.pipeline import Pipeline\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "#- Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#- Data visualisation\n",
    "import lux\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "\n",
    "#- Preprocessing for model fitting\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#- ML models\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "#- Model assessement\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, roc_auc_score, roc_curve, confusion_matrix, auc, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Dataset</b>\n",
    "*If you have several data sets, you need to duplicate the cells below. If your dataset is in a different format, create a new cell and write the code to import it. Don't forget Chat GPT and Google are your friends. ;-)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If your data is in a CSV file use the cell below to import it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \".csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "raw_data = data.copy()\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If your data is in an excel file use the cell below to import it instead.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \".xlsx\"\n",
    "# data = pd.read_excel(data_path)\n",
    "# data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Configurations</b>\n",
    "*Set any global configurations such as random seed for reproducibility, backend settings for computation libraries, and any project-specific parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "#- Show all the column\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#- Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"test_mlflow_setup\")      # This is just to test if the mlflow server is working\n",
    "try:\n",
    "    mlflow.log_param(\"test_param\", \"test_value\")\n",
    "    print(\"MLflow is configured correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"MLflow configuration failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 2:</b> Data inspection and cleanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Inspection</b>\n",
    "*The goal of data inspection is to understand the dataset's characteristics, quality, and any potential issues that need addressing before analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_inspection(df, visualize_nulls=True, detailed_summary=True, check_duplicates=True):\n",
    "    \"\"\"\n",
    "    Perform a basic inspection of a pandas DataFrame including size, data types,\n",
    "    null values, a heatmap of missing values, duplicate rows, and a summary table with optional\n",
    "    detailed analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame.\n",
    "    - visualize_nulls: Bool, if True, visualizes null values using a heatmap.\n",
    "    - detailed_summary: Bool, if True, includes detailed analysis in the summary table.\n",
    "    - check_duplicates: Bool, if True, checks and reports the number of duplicate rows.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing a summary of the inspection.\n",
    "    \"\"\"\n",
    "    print(f'Number of rows: {df.shape[0]}\\nNumber of columns: {df.shape[1]}\\n')\n",
    "    print('Data types:\\n', df.dtypes)\n",
    "    \n",
    "    null_counts = df.isnull().sum()\n",
    "    print('\\nColumns with null values:\\n', null_counts[null_counts > 0])\n",
    "\n",
    "    if check_duplicates:\n",
    "        duplicate_rows = df.duplicated().sum()\n",
    "        print(f'\\nNumber of duplicate rows: {duplicate_rows}\\n')\n",
    "\n",
    "    if visualize_nulls:\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        sns.heatmap(df.isna(), cbar=True, cmap='viridis')\n",
    "        plt.title('Heatmap of Missing Values')\n",
    "        plt.show()\n",
    "\n",
    "    summary_table = pd.DataFrame({\n",
    "        \"Unique_values\": df.nunique(),\n",
    "        \"Data_type\": df.dtypes,\n",
    "        \"Null_count\": df.isnull().sum(),\n",
    "        \"Null_percentage\": (df.isnull().sum() / df.shape[0] * 100).round(2)\n",
    "    })\n",
    "\n",
    "    if detailed_summary:\n",
    "        if 'object' in df.dtypes.values:\n",
    "            summary_table['Most_common'] = df.apply(lambda x: x.value_counts().idxmax() if x.dtypes == 'object' else 'unknown')\n",
    "        for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "            summary_table.loc[col, 'Mean'] = df[col].mean()\n",
    "            summary_table.loc[col, 'Std'] = df[col].std()\n",
    "            summary_table.loc[col, 'Min'] = df[col].min()\n",
    "            summary_table.loc[col, 'Max'] = df[col].max()\n",
    "\n",
    "    return summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the following cell, modify the parameters according to the particularities of your database. For example, if you have intentional duplicates, it is not necessary to check them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_inspection(data, visualize_nulls=True, detailed_summary=True, check_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(df, plot=True):\n",
    "    \"\"\"\n",
    "    Identify outliers in a pandas DataFrame using Z-score (values more than 3 standard deviations from \n",
    "    the mean) and Interquartile Range (IQR) methods (values below Q1 - 1.92*IQR or above Q3 + 1.92*IQR).\n",
    "    Outliers identified by both methods are combined to provide a comprehensive overview of outliers \n",
    "    in each numeric feature.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): A pandas DataFrame containing the data to be analyzed. Only numeric\n",
    "      columns will be considered for outlier detection.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where each key is the name of a numeric feature in the DataFrame. Each value\n",
    "      is another dictionary containing two keys: 'num_outliers', which is the number of unique outliers\n",
    "      identified in the feature, and 'outliers_index', an array of indices of these outliers.\n",
    "    \"\"\"\n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for column in df.select_dtypes(include=np.number).columns:  # Focus on numeric columns\n",
    "        # Calculate Z-scores\n",
    "        z_scores = np.abs(stats.zscore(df[column].dropna()))\n",
    "        z_outliers = np.where(z_scores > 3)[0]\n",
    "        \n",
    "        # Calculate IQR\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        iqr_outliers = df[(df[column] < (Q1 - 1.96 * IQR)) | (df[column] > (Q3 + 1.96 * IQR))].index\n",
    "        \n",
    "        # Combine unique outliers from both methods\n",
    "        combined_outliers = np.union1d(z_outliers, iqr_outliers)\n",
    "        \n",
    "        # Optionally plot\n",
    "        if plot:\n",
    "            print(column)\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.boxplot(x=df[column])\n",
    "            plt.title(f'Boxplot of {column} (Outliers highlighted)')\n",
    "            plt.show()\n",
    "\n",
    "        # Summary\n",
    "        outlier_summary[column] = {\n",
    "            'num_outliers': len(combined_outliers),\n",
    "            'outliers_index': combined_outliers\n",
    "        }\n",
    "    \n",
    "    return outlier_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This function does not modify the original DataFrame. I recommend you to inspect the identified outliers and decide on appropriate handling methods such as removal, replacement, or keeping them as is, depending on the analysis requirements and domain knowledge.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_info = identify_outliers(data)\n",
    "print(outlier_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Cleaning</b>\n",
    "*Following the inspection, data cleaning aims to rectify issues identified, improving the dataset's quality and making it suitable for further analysis and modeling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Feature Selection\n",
    "\n",
    "# data.drop(column=[], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Remove duplicates\n",
    "\n",
    "# data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data):\n",
    "    #- Handle missing values\n",
    "    data['numerical_column'].fillna(data['numerical_column'].mean(), inplace=True)\n",
    "    data['categorical_column'].fillna(data['categorical_column'].mode()[0], inplace=True)\n",
    "    \n",
    "    #- Correct data type\n",
    "    data['numeric_column_as_string'] = pd.to_numeric(data['numeric_column_as_string'], errors='coerce')\n",
    "    data['date_column'] = pd.to_datetime(data['date_column'])\n",
    "\n",
    "    #- Binning\n",
    "    #- Fixed width binning\n",
    "    data['value_bin'] = pd.cut(data['column_to_bin'], bins=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "    #- Qauntile based binning\n",
    "    data['value_quantile_bin'] = pd.qcut(data['column_to_bin'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "    #- Custom binning\n",
    "    bins = []\n",
    "    data['value_custom_bin'] = pd.cut(data['column_to_bin'], bins=bins, labels=[\"Low\", \"Medium\", \"High\"], right=False)\n",
    "\n",
    "    #- Turn categorical variables into numerical variables\n",
    "\n",
    "    #- Label encoding\n",
    "    # Define a mapping of categories to numerical values\n",
    "    checking_mapping = {'None': 0, 'little': 1, 'moderate': 2, 'rich': 3}\n",
    "    # Map the categories to their numerical equivalents\n",
    "    data['col_name'] = data['col_name_numb'].map(checking_mapping)\n",
    "\n",
    "    #- One-hot encoding of categorical variables\n",
    "    categorical_cols = []       # Add the columns of interest\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cleaning(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>When deciding between one-hot encoding and simple encoding (label encoding) for categorical data, consider the nature of your categories:\n",
    "\n",
    "- One-Hot Encoding: Use this when your categorical variable does not have a meaningful order or hierarchy. One-hot encoding creates a new binary column for each category, which is ideal for nominal data (e.g., color, city names). This approach prevents the model from assuming a natural ordering between categories, which is helpful to avoid misleading interpretations.\n",
    "\n",
    "- Label Encoding: Use this when your categorical variable has a meaningful order or ranking (ordinal data). Label encoding assigns a unique integer to each category, preserving the order. However, be cautious, as this can introduce unintended ordinal relationships that may not exist in the data.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also don't apply the cleaning of the target feature in the data_cleaning function as we will apply this function to the prediction dataset (which won't have the target column) too. Do the transformation for the target column in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Handle missing values\n",
    "data['numerical_column'].fillna(data['numerical_column'].mean(), inplace=True)\n",
    "data['categorical_column'].fillna(data['categorical_column'].mode()[0], inplace=True)\n",
    "\n",
    "#- Correct data type\n",
    "data['numeric_column_as_string'] = pd.to_numeric(data['numeric_column_as_string'], errors='coerce')\n",
    "data['date_column'] = pd.to_datetime(data['date_column'])\n",
    "\n",
    "#- Binning\n",
    "#- Fixed width binning\n",
    "data['value_bin'] = pd.cut(data['column_to_bin'], bins=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "#- Qauntile based binning\n",
    "data['value_quantile_bin'] = pd.qcut(data['column_to_bin'], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
    "#- Custom binning\n",
    "bins = []\n",
    "data['value_custom_bin'] = pd.cut(data['column_to_bin'], bins=bins, labels=[\"Low\", \"Medium\", \"High\"], right=False)\n",
    "\n",
    "#- Turn categorical variables into numerical variables\n",
    "#- Label encoding\n",
    "# Define a mapping of categories to numerical values\n",
    "checking_mapping = {'None': 0, 'little': 1, 'moderate': 2, 'rich': 3}\n",
    "# Map the categories to their numerical equivalents\n",
    "data['col_name'] = data['col_name_numb'].map(checking_mapping)\n",
    "#- One-hot encoding of categorical variables\n",
    "categorical_cols = []       # Add the columns of interest\n",
    "data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Outlier Treatment\n",
    "\n",
    "# for col, info in outlier_info.items():\n",
    "#     data = data.drop(index=info['outliers_index'], inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 3:</b> Exploratory Data Analysis (problem navigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Univariate analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_univariate_analysis(df, col_name, visualize=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Perform univariate analysis on a numerical column with visualization\n",
    "    options using a Matplotlib native color palette focusing on red, yellow, and black.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame containing the data.\n",
    "    col_name (str): Name of the numerical column to analyze.\n",
    "    visualize (bool): Whether to visualize the analysis (default=True).\n",
    "    **kwargs: Additional keyword arguments to customize the visualisation colors.\n",
    "\n",
    "    Returns:\n",
    "    pandas.Series: Descriptive statistics of the column.\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{col_name}' not found in the DataFrame.\")\n",
    "\n",
    "    # 1. Descriptive Statistics\n",
    "    descriptive_stats = df[col_name].describe()\n",
    "    print(\"\\n\"*2, col_name, \"\\n\"*2, descriptive_stats)\n",
    "\n",
    "    if visualize:\n",
    "        # Custom color palette\n",
    "        custom_colors = ['red', 'yellow', 'black']  # Red, Yellow, Black\n",
    "        color = kwargs.get('hist_color', custom_colors[0])  # Use red as default\n",
    "        \n",
    "        # 2. Histogram with KDE\n",
    "        plt.figure(figsize=kwargs.get('figsize', (12, 6)))\n",
    "        sns.histplot(df[col_name], kde=True, bins=kwargs.get('bins', 30),\n",
    "                     color=color,\n",
    "                     kde_kws={'bw_adjust': kwargs.get('bw_adjust', 1)},\n",
    "                     line_kws={'color': custom_colors[2], 'lw': 2})  # Use black for KDE line\n",
    "        plt.title(f'Histogram and KDE of {col_name}')\n",
    "        plt.xlabel(col_name)\n",
    "        plt.ylabel('')\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5, color=custom_colors[1])  # Use yellow for grid lines\n",
    "        plt.show()\n",
    "\n",
    "        # 3. Boxplot\n",
    "        plt.figure(figsize=kwargs.get('figsize', (12, 6)))\n",
    "        sns.boxplot(x=df[col_name], color=kwargs.get('boxplot_color', custom_colors[1]))  # Use yellow for boxplot\n",
    "        plt.title(f'Boxplot of {col_name}')\n",
    "        plt.xlabel(col_name)\n",
    "        plt.grid(True, linestyle='--', linewidth=0.5, color=custom_colors[2])  # Use black for grid lines\n",
    "        plt.show()\n",
    "\n",
    "    return descriptive_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quant_col in []:        # Add in the brackets the name of the quantitative variables in your dataset that you want to visualize.\n",
    "    quant_univariate_analysis(data, quant_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qual_univariate_analysis(df, col_name, palette=\"viridis\", show_grid=True, figsize=(12, 6)):\n",
    "    \"\"\"\n",
    "    Performs and visualizes a univariate analysis for a qualitative (categorical) variable,\n",
    "    highlighting and annotating the most and least common categories.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The pandas DataFrame containing the data.\n",
    "    - col_name (str): The name of the column to analyze.\n",
    "    - palette (str, optional): Color palette for the plots. Defaults to 'viridis'.\n",
    "    - show_grid (bool, optional): Whether to show the grid in the bar plot. Defaults to True.\n",
    "    - figsize (tuple, optional): Figure size for the plots. Defaults to (12, 6).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Frequency Table\n",
    "    freq_table = df[col_name].value_counts()\n",
    "    # Percentage Table\n",
    "    percent_table = df[col_name].value_counts(normalize=True) * 100\n",
    "    combined_table = pd.DataFrame({'Frequency': freq_table, 'Percentage': percent_table})\n",
    "    print(\"\\n\"*2, col_name, \"\\n\"*2, combined_table)\n",
    "\n",
    "    # Identify most and least common categories\n",
    "    most_common = freq_table.idxmax()\n",
    "    least_common = freq_table.idxmin()\n",
    "\n",
    "    # Bar Plot with Highlighting\n",
    "    plt.figure(figsize=figsize)\n",
    "    barplot = sns.countplot(y=df[col_name], order=freq_table.index, palette=palette)\n",
    "    \n",
    "    # Highlighting\n",
    "    for patch in barplot.patches:\n",
    "        if patch.get_y() == freq_table.index.get_loc(most_common):\n",
    "            patch.set_facecolor('green')  # Highlight most common category in green\n",
    "        elif patch.get_y() == freq_table.index.get_loc(least_common):\n",
    "            patch.set_facecolor('red')  # Highlight least common category in red\n",
    "    \n",
    "    # Annotations\n",
    "    plt.title(f'Bar Plot of {col_name}')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(col_name)\n",
    "    if show_grid:\n",
    "        plt.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Adding annotations for the most and least common categories\n",
    "    plt.text(freq_table.max(), freq_table.index.get_loc(most_common), 'Most common', fontsize=12, va='center')\n",
    "    plt.text(freq_table.min(), freq_table.index.get_loc(least_common), 'Least common', fontsize=12, va='center')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return combined_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qual_col in []:         # Add in the brackets the name of the qualitative variables in your dataset that you want to visualize.\n",
    "    qual_univariate_analysis(data, qual_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Bivariate analysis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Calculate the correlation matrix\n",
    "correlation_cols = []       # Add in the brackets the name of the the columns you want to visualize for correlation. Make sure these columns are numeric. You can numerize the qualitative columns back in the preprocessing step.\n",
    "correlation = data[correlation_cols].corr(method='pearson')\n",
    "\n",
    "#- Visualize the correlation matrix\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.figure.set_size_inches(10, 10)\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(correlation, cmap=cmap, mask=mask, square=True, linewidths=.5, \n",
    "            annot=True, annot_kws={'size':14})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_corr = correlation[(correlation > 0.7) | (correlation < -0.7)]\n",
    "print(\"Strong correlations:\\n\", strong_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 4:</b> Feature Engineering\n",
    "*The Feature engineering phase involves transforming raw data into meaningful features that effectively represent the underlying problem.*  \n",
    "*<b>Feature Creation:</b>* Develop new features from the existing data to better capture the underlying patterns.  \n",
    "*<b>Feature Transformation:</b>* Apply transformations (e.g., scaling, encoding) to make the data suitable for modeling.  \n",
    "*<b>Feature Selection:</b>* Use statistical tests and selection algorithms to reduce dimensionality and focus on relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_feature_eng(df):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = do_feature_eng(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 5:</b> Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Define Features and Target</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Features:\n",
    "features_cols = []\n",
    "X = data[features_cols]\n",
    "\n",
    "#- Target:\n",
    "y = data[target_name]       # Replace target_name with the name of your target variable in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Scaling</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Train validation and Test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_X_train, X_test, pre_y_train ,y_test = train_test_split(X, y, test_size=0.20, random_state=SEED, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(pre_X_train, pre_y_train, test_size=0.25, random_state=42, stratify=pre_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the X training database: \", X_train.shape)\n",
    "print(\"Size of the X validation database: \", X_val.shape)\n",
    "print(\"Size of the X testing database: \", X_test.shape)\n",
    "print(\"Size of the y training database: \", y_train.shape)\n",
    "print(\"Size of the y validation database: \", y_val.shape)\n",
    "print(\"Size of the y testing database: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Dealing with inbalanced data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=SEED)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 6:</b> Training & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Fitting a base model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Start a new MLflow run\n",
    "mlflow.set_experiment(\"baseline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    #- Define the baseline model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=SEED, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline to training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    y_pred_proba = pipeline.predict_proba(X_val)[:, 1]  # For ROC and PR curves\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\")\n",
    "    \n",
    "    # Log metrics with MLflow\n",
    "    mlflow.log_metric('accuracy', accuracy)\n",
    "    mlflow.log_metric('precision', precision)\n",
    "    mlflow.log_metric('recall', recall)\n",
    "    mlflow.log_metric('f1_score', f1)\n",
    "    \n",
    "    # Generate ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\", color=\"blue\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"red\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"roc_curve.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Log ROC Curve as artifact\n",
    "    mlflow.log_artifact(\"roc_curve.png\")\n",
    "    \n",
    "    # Generate Precision-Recall Curve\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall_vals, precision_vals, label=\"PR Curve\", color=\"green\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"pr_curve.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Log PR Curve as artifact\n",
    "    mlflow.log_artifact(\"pr_curve.png\")\n",
    "    \n",
    "    # Save the model and log it with MLflow\n",
    "    mlflow.sklearn.log_model(pipeline, \"baseline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Fitting an hyper-optimized model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Start a new MLflow run\n",
    "mlflow.set_experiment(\"optimized_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Define classifiers and their parameter grids\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(),\n",
    "    \"Support Vector Machine\": SVC(probability=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_mlflow_tuner(trial):\n",
    "    \"\"\"\n",
    "    Function to optimize hyperparameters of a wide range of classifiers using Optuna \n",
    "    and log detailed results with MLflow. Combines advanced feature engineering, \n",
    "    standardized scaling, and classifier evaluation with detailed metrics and visualizations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    trial : optuna.trial.Trial\n",
    "        The trial object provided by Optuna to suggest hyperparameters and track the results.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The main evaluation metric (accuracy) of the model on the validation set.\n",
    "    \"\"\"\n",
    "    global best_model\n",
    "\n",
    "    # Start an MLflow run for this trial\n",
    "    with mlflow.start_run():\n",
    "        # Select a classifier\n",
    "        model_name = trial.suggest_categorical(\"model\", list(classifiers.keys()))\n",
    "        classifier = classifiers[model_name]\n",
    "\n",
    "        # Suggest hyperparameters specific to the selected classifier\n",
    "        if model_name == \"Logistic Regression\":\n",
    "            solver = trial.suggest_categorical(\"solver\", [\"newton-cg\", \"lbfgs\", \"saga\", \"liblinear\"])\n",
    "            C = trial.suggest_loguniform(\"C\", 0.001, 100.0)\n",
    "            classifier.set_params(solver=solver, C=C)\n",
    "\n",
    "        elif model_name == \"Random Forest\":\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000, step=50)\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 2, 100, step=5)\n",
    "            min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 50, step=5)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 50, step=5)\n",
    "            classifier.set_params(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "        elif model_name == \"Gaussian Naive Bayes\":\n",
    "            var_smoothing = trial.suggest_loguniform(\"var_smoothing\", 1e-12, 1e-2)\n",
    "            classifier.set_params(var_smoothing=var_smoothing)\n",
    "\n",
    "        elif model_name == \"K-Nearest Neighbors\":\n",
    "            n_neighbors = trial.suggest_int(\"n_neighbors\", 1, 50)\n",
    "            weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
    "            algorithm = trial.suggest_categorical(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"])\n",
    "            p = trial.suggest_int(\"p\", 1, 5)  # Minkowski distance metric\n",
    "            classifier.set_params(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, p=p)\n",
    "\n",
    "        elif model_name == \"Decision Tree\":\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 2, 100, step=5)\n",
    "            min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 50, step=5)\n",
    "            min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 50, step=5)\n",
    "            criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "            classifier.set_params(\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                criterion=criterion\n",
    "            )\n",
    "\n",
    "        elif model_name == \"XGBoost\":\n",
    "            max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "            learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.001, 1.0)\n",
    "            n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000, step=50)\n",
    "            subsample = trial.suggest_uniform(\"subsample\", 0.5, 1.0)\n",
    "            colsample_bytree = trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0)\n",
    "            gamma = trial.suggest_loguniform(\"gamma\", 1e-8, 10.0)\n",
    "            classifier.set_params(\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=learning_rate,\n",
    "                n_estimators=n_estimators,\n",
    "                subsample=subsample,\n",
    "                colsample_bytree=colsample_bytree,\n",
    "                gamma=gamma\n",
    "            )\n",
    "\n",
    "        elif model_name == \"Support Vector Machine\":\n",
    "            kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"])\n",
    "            C = trial.suggest_loguniform(\"C\", 0.001, 100.0)\n",
    "            classifier.set_params(kernel=kernel, C=C)\n",
    "\n",
    "        # Build the pipeline\n",
    "        pipeline = Pipeline([\n",
    "            (\"feature_engineering\", FunctionTransformer(do_feature_eng, validate=False)),  # Replace resize with Feature Engineering\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"classifier\", classifier)\n",
    "        ])\n",
    "\n",
    "        # Train the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred, average='weighted')\n",
    "        recall = recall_score(y_val, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "        # Log metrics in MLflow\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "\n",
    "        # Log hyperparameters\n",
    "        params = classifier.get_params()\n",
    "        for param_name, param_value in params.items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "\n",
    "        # Log the confusion matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.savefig(\"confusion_matrix.png\")\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save the model if it's the best one found so far\n",
    "        if trial.number == 0 or accuracy > study.best_value:\n",
    "            best_model = pipeline\n",
    "\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optuna_mlflow_tuner, n_trials=15)\n",
    "\n",
    "print(\"\\nBest hyperparameters found:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Testing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model, X_test, y_test, class_names=None):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on the test set and log metrics and visualizations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The trained machine learning model (pipeline or standalone model).\n",
    "    X_test : array-like\n",
    "        The test set features.\n",
    "    y_test : array-like\n",
    "        The true labels of the test set.\n",
    "    class_names : list, optional\n",
    "        List of class names for visualization and metrics. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Generate the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Return the metrics as a dictionary\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_on_test(best_model, X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 7:</b> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X_input, class_names=None):\n",
    "    \"\"\"\n",
    "    Make predictions using a trained model and optionally decode class labels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The trained machine learning model (pipeline or standalone model).\n",
    "    X_input : array-like\n",
    "        The input data for which predictions need to be made.\n",
    "    class_names : list, optional\n",
    "        List of class names to decode numerical predictions into labels. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : array-like\n",
    "        Predicted class labels or probabilities.\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_input)\n",
    "\n",
    "    # Optionally map numerical predictions to class names\n",
    "    if class_names is not None:\n",
    "        y_pred_decoded = [class_names[label] for label in y_pred]\n",
    "        return y_pred_decoded\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Example usage:\n",
    "X_to_predict = [[]]\n",
    "predictions = make_predictions(best_model, X_to_predict, class_names)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prob_predictions(model, X_input, class_names=None):\n",
    "    \"\"\"\n",
    "    Make probability predictions using a trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        The trained machine learning model (pipeline or standalone model).\n",
    "    X_input : array-like\n",
    "        The input data for which probability predictions need to be made.\n",
    "    class_names : list, optional\n",
    "        List of class names to associate probabilities with each class. Default is None.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    probabilities : array-like\n",
    "        Predicted probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Make probability predictions\n",
    "    y_proba = model.predict_proba(X_input)\n",
    "\n",
    "    # Optionally pair probabilities with class names\n",
    "    if class_names is not None:\n",
    "        proba_dicts = [{class_names[i]: prob for i, prob in enumerate(probs)} for probs in y_proba]\n",
    "        return proba_dicts\n",
    "\n",
    "    return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "proba_predictions = make_prob_predictions(best_model, X_to_predict, class_names)\n",
    "print(\"Probability Predictions:\", proba_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_2 = \".csv\"\n",
    "data_2 = pd.read_csv(data_path_2)\n",
    "raw_data_2 = data_2.copy()\n",
    "data_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_inspection(data_2, visualize_nulls=True, detailed_summary=True, check_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = [\"feature1\", \"feature2\", \"feature3\"]  # Replace with actual feature names\n",
    "X_to_predict = data_2[X_features]\n",
    "data_2 = data_cleaning(X_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = make_predictions(best_model, X_to_predict, class_names=class_names)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: If you want probabilities instead of class labels\n",
    "probability_predictions = make_prob_predictions(best_model, X_to_predict, class_names=class_names)\n",
    "print(\"\\nProbability Predictions:\")\n",
    "print(probability_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the original data\n",
    "results_df = raw_data_2.copy()  # Copy the original data for context\n",
    "results_df[\"Predicted Class\"] = predictions  # Add predicted classes as a new column\n",
    "\n",
    "# Optional: Add probabilities as new columns (if applicable)\n",
    "if probability_predictions:\n",
    "    for class_name in class_names:\n",
    "        results_df[f\"Probability ({class_name})\"] = [prob[class_name] for prob in probability_predictions]\n",
    "\n",
    "results_file_path = \"predictions_results.csv\"\n",
    "results_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "print(f\"Results exported successfully to {results_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
